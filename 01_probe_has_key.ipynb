{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ imports OK \n"
     ]
    }
   ],
   "source": [
    "import sys, pathlib, gymnasium as gym, numpy as np, torch\n",
    "import transformer_lens as tl                 # TL 2.15 +\n",
    "\n",
    "repo_root = pathlib.Path(\n",
    "    \"/Users/benjaminhawken/Library/CloudStorage/OneDrive-Personal/AI Research/mechinterp-sprint/DecisionTransformerInterpretability\"\n",
    ")\n",
    "sys.path.extend([str(repo_root), str(repo_root / \"src\")])\n",
    "\n",
    "from models.trajectory_transformer import DecisionTransformer\n",
    "from config import EnvironmentConfig, TransformerModelConfig\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"✅ imports OK \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEQ_LEN = 15                       # timesteps kept from each traj\n",
    "\n",
    "model_cfg = TransformerModelConfig(\n",
    "    d_model  = 128,\n",
    "    n_heads  = 4,\n",
    "    d_mlp    = 512,\n",
    "    n_layers = 2,\n",
    "    n_ctx    = 3 * SEQ_LEN - 1,     # 44 (must satisfy (n_ctx-2)%3==0)\n",
    "    activation_fn = \"gelu\",\n",
    "    state_embedding_type = \"flat\",  # ← matches patched linear layer\n",
    "    time_embedding_type  = \"embedding\",\n",
    "    seed  = 1,\n",
    "    device = device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obs_space = gym.spaces.Box(0, 255, shape=(148,), dtype=np.float32)\n",
    "act_space = gym.spaces.Discrete(7)\n",
    "\n",
    "env_cfg = EnvironmentConfig(\n",
    "    env_id            = \"MiniGrid-DoorKey-8x8-v0\",\n",
    "    observation_space = obs_space,\n",
    "    action_space      = act_space,\n",
    "    max_steps         = 160,\n",
    "    device            = device,\n",
    "    one_hot_obs = False, img_obs = False, fully_observed = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DT weights loaded\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTransformer(\n",
    "    environment_config = env_cfg,\n",
    "    transformer_config = model_cfg,\n",
    ").to(device)\n",
    "\n",
    "state_dict = torch.load(\"dt_dti_flat.pth\", map_location=device)\n",
    "missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "assert not missing and not unexpected, (missing, unexpected)\n",
    "model.eval()\n",
    "print(\"✅ DT weights loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: torch.Size([1, 44, 128])\n",
      "logits: torch.Size([1, 44, 128])\n"
     ]
    }
   ],
   "source": [
    "# --- 1. dummy 15‑step trajectory ------------------------------------------------\n",
    "import torch, numpy as np\n",
    "\n",
    "B, S = 1, 15\n",
    "states  = torch.randn(B, S, 148, device=model.transformer.cfg.device)        # (1,15,148)\n",
    "actions = torch.zeros(B, S-1, 1, dtype=torch.long, device=device)\n",
    "rtgs    = torch.zeros (B, S,   1, device=states.device)\n",
    "tsteps  = torch.arange(S, device=states.device)[None, :, None]               # (1,15,1)\n",
    "\n",
    "tokens = model.to_tokens(states, actions, rtgs, tsteps)                      # (1,44,128)\n",
    "print(\"tokens:\", tokens.shape)\n",
    "\n",
    "# --- forward through the HookedTransformer -------------------------------------\n",
    "logits = model.transformer(tokens)                     # (1,44,128)\n",
    "print(\"logits:\", logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache‑keys (first 8):\n",
      "  • hook_embed torch.Size([1, 44, 128])\n",
      "  • hook_pos_embed torch.Size([1, 44, 128])\n",
      "  • blocks.0.hook_resid_pre torch.Size([1, 44, 128])\n",
      "  • blocks.0.attn.hook_q torch.Size([1, 44, 4, 32])\n",
      "  • blocks.0.attn.hook_k torch.Size([1, 44, 4, 32])\n",
      "  • blocks.0.attn.hook_v torch.Size([1, 44, 4, 32])\n",
      "  • blocks.0.attn.hook_attn_scores torch.Size([1, 4, 44, 44])\n",
      "  • blocks.0.attn.hook_pattern torch.Size([1, 4, 44, 44])\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import ActivationCache\n",
    "\n",
    "logits, cache = model.transformer.run_with_cache(\n",
    "    tokens, return_type=\"logits\"\n",
    ")   \n",
    "\n",
    "print(\"cache‑keys (first 8):\")\n",
    "for k in list(cache)[:8]:\n",
    "    print(\"  •\", k, cache[k].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample hook names: hook_embed blocks.0.attn.hook_z blocks.1.mlp.hook_post\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample hook names:\u001b[39m\u001b[38;5;124m\"\u001b[39m, hook_embed, hook_z_L0, hook_mlp_out_L1)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 2) Verify that those hooks really exist in the Decision‑Transformer ----\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m all_hook_names \u001b[38;5;241m=\u001b[39m {hp\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m hp \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mhook_points}\n\u001b[1;32m     13\u001b[0m missing \u001b[38;5;241m=\u001b[39m [h \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m [hook_embed, hook_z_L0, hook_mlp_out_L1] \u001b[38;5;28;01mif\u001b[39;00m h \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_hook_names]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThese hooks weren’t found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not iterable"
     ]
    }
   ],
   "source": [
    "from transformer_lens.utils import get_act_name\n",
    "assert isinstance(model.transformer, tl.HookedTransformer)\n",
    "\n",
    "# 1) Build a few canonical hook names ------------------------------------\n",
    "hook_embed     = get_act_name(\"embed\")                 # 'hook_embed'\n",
    "hook_z_L0      = get_act_name(\"z\",      0)             # 'blocks.0.attn.hook_z'\n",
    "hook_resid_L1  = get_act_name(\"resid_pre\", 1)          # 'blocks.1.hook_resid_pre'\n",
    "hook_mlp_out_L1= get_act_name(\"post\",   1)             # 'blocks.1.mlp.hook_post'\n",
    "print(\"Sample hook names:\", hook_embed, hook_z_L0, hook_mlp_out_L1)\n",
    "\n",
    "# 2) Verify that those hooks really exist in the Decision‑Transformer ----\n",
    "all_hook_names = {hp.name for hp in model.transformer.hook_points}\n",
    "missing = [h for h in [hook_embed, hook_z_L0, hook_mlp_out_L1] if h not in all_hook_names]\n",
    "assert not missing, f\"These hooks weren’t found: {missing}\"\n",
    "print(\"✅  All sample hooks are present in model.hook_points\")\n",
    "\n",
    "# 3) Do a tiny intervention: zero OV of L0H0 and watch logits change -----\n",
    "layer, head = 0, 0\n",
    "\n",
    "def zero_head_z(z, hook):\n",
    "    # z shape: [batch, pos, head, d_head] in TL 2.15\n",
    "    z[..., head, :] = 0\n",
    "    return z\n",
    "\n",
    "with model.transformer.hooks() as h:\n",
    "    h.add_hook(hook_z_L0, zero_head_z)     # use the name we just built\n",
    "    logits_ablate = model.transformer(tokens)\n",
    "\n",
    "delta = (logits_ablate - logits).abs().max().item()\n",
    "print(f\"Δ max‑logit after ablating L{layer}H{head}: {delta:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dti_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
